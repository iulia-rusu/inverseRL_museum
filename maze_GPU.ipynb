{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gymnasium & Minigrid imports\n",
    "import gymnasium as gym  # Correct way to import Gymnasium\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.constants import DIR_TO_VEC\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.actions import Actions\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "\n",
    "\n",
    "\n",
    "from gymnasium.utils.play import play\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "from scipy.ndimage import binary_dilation\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import matrix_power\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mask.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mask, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reward_struct.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reward_struct, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r museum_mask_03m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = museum_mask_03m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.flipud(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"images/walkability_mask.npy\", walkability_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (713913533.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.goal_pos =\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class SimplestEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self, \n",
    "        agent_start_pos=(7, 21), \n",
    "        agent_start_dir=0, \n",
    "        max_steps=700,\n",
    "        mask=None, \n",
    "        **kwargs,\n",
    "    ):\n",
    "        assert mask is not None, \"You must provide a walkability mask\"\n",
    "        height, width = mask.shape\n",
    "        self.mask = mask\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        self.goal_pos = \n",
    "\n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=max(width, height),\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"Museum\"\n",
    "\n",
    "\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if not self.mask[y, x]:\n",
    "                    self.grid.set(x, y, Wall())\n",
    "        #place agent\n",
    "        \n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos #check this\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "        print(f\"[DEBUG] Placing agent at {self.agent_pos} facing {self.agent_dir}\")\n",
    "        \n",
    "        # Do not call self.place_agent() → we already set it\n",
    "        self.mission = \"Museum\"\n",
    "        print(f\"[DEBUG] Initialized grid {self.width}x{self.height}, agent at {self.agent_pos}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] mask.shape = (37, 69), self.width = 69, self.height = 37\n"
     ]
    }
   ],
   "source": [
    "env = SimpleEnv(render_mode=\"human\", mask = mask)\n",
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Placing agent at (7, 21) facing 0\n",
      "[DEBUG] Initialized grid 69x37, agent at (7, 21)\n"
     ]
    }
   ],
   "source": [
    "test_env = SimplestEnv(\n",
    "     # (x, y) format → left edge, near bottom           # Facing up (0 = up, 1 = right, 2 = down, 3 = left)\n",
    "    render_mode=\"human\",\n",
    "    mask=mask\n",
    ")\n",
    "test_env.reset(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define reward structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias #import\n",
      "no stored variable or alias from\n",
      "no stored variable or alias computed\n",
      "no stored variable or alias notebook\n"
     ]
    }
   ],
   "source": [
    "%store -r reward_grid_03  #import from computed notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1]], shape=(37, 69))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_grid_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flip\n",
    "reward_struct = np.flipud(reward_grid_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1]], shape=(37, 69))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "            self, \n",
    "            agent_start_pos=(1, 30), # bottom left entrance ( 1, 30) top right corner (63, 5)\n",
    "            agent_start_dir=0, \n",
    "            max_steps=50,\n",
    "            mask = None, \n",
    "            reward_mask = None,\n",
    "            \n",
    "            **kwargs,\n",
    "    ):\n",
    "        \n",
    "    \n",
    "        height, width = mask.shape\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        self.agent_dir = 0 #facing up always\n",
    "        # self.goal_pos = (30, 15)\n",
    "        self.mask = mask\n",
    "        self.reward_mask = reward_mask\n",
    "        self.visited_reward_0 = set()  # holds (x, y) tuples\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            # grid_size=max(width, height),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Restore correct values\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"Museum\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "\n",
    "        for y in range(self.mask.shape[0]):\n",
    "            for x in range(self.mask.shape[1]):\n",
    "                if not self.mask[y, x]:\n",
    "                    self.grid.set(x, y, Wall())\n",
    "\n",
    "\n",
    "\n",
    "        #place goal\n",
    "        # self.put_obj(Goal(), 30, 15)\n",
    "\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos #check this\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"Museum\"\n",
    "\n",
    "        # print(f\"agent_start = {self.agent_start_pos}\")\n",
    "        print(f\"[DEBUG] Initialized grid {self.width}x{self.height}, agent at {self.agent_pos}\")\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = map(int, self.agent_pos)\n",
    "\n",
    "        # Define absolute movement\n",
    "        if action == 0:      # stay\n",
    "            dx, dy = 0, 0\n",
    "        elif action == 1:    # up\n",
    "            dx, dy = 0, -1\n",
    "        elif action == 2:    # down\n",
    "            dx, dy = 0, 1\n",
    "        elif action == 3:    # left\n",
    "            dx, dy = -1, 0\n",
    "        elif action == 4:    # right\n",
    "            dx, dy = 1, 0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "\n",
    "        new_x, new_y = int(x + dx), int(y + dy)\n",
    "\n",
    "        # Stay in bounds\n",
    "        if not (0 <= new_x < self.width and 0 <= new_y < self.height):\n",
    "            new_x, new_y = x, y\n",
    "\n",
    "        # Check for wall\n",
    "        target_cell = self.grid.get(new_x, new_y)\n",
    "        if target_cell is not None and not target_cell.can_overlap():\n",
    "            new_x, new_y = x, y  # can't move into wall\n",
    "\n",
    "        self.agent_pos = (new_x, new_y)\n",
    "\n",
    "        # --- REWARD LOGIC ---\n",
    "        if self.reward_mask[new_y, new_x] == 0:\n",
    "            if (new_x, new_y) in self.visited_reward_0:\n",
    "                reward = -1  # revisited exhibit\n",
    "            else:\n",
    "                reward = 0   # first time visit\n",
    "                self.visited_reward_0.add((new_x, new_y))\n",
    "        else:\n",
    "            reward = self.reward_mask[new_y, new_x]\n",
    "\n",
    "\n",
    "        obs = self.gen_obs()\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    \n",
    "    def count_states(self):\n",
    "        free_cells = sum(1 for x in range(self.grid.width)\n",
    "                      for y in range(self.grid.height)\n",
    "                      if not self.grid.get(x, y)) * 4\n",
    "        return free_cells \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Initialized grid 69x37, agent at (1, 30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = SimpleEnv(render_mode=\"human\", mask = mask, reward_mask= reward_struct)\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_to_state_index(pos=None):\n",
    "    \"\"\"Map (x, y) to state index assuming row-major order.\"\"\"\n",
    "    if pos is None:\n",
    "        x, y = env.agent_pos\n",
    "    else:\n",
    "        x, y = pos\n",
    "    return y * env.width + x\n",
    "\n",
    "def state_index_to_position(idx):\n",
    "    \"\"\"Map scalar state index to (x, y) position.\"\"\"\n",
    "    x = idx % env.width\n",
    "    y = idx // env.width\n",
    "    return x, y\n",
    "\n",
    "def find_state_indexes():\n",
    "    \"\"\"Return all non-wall, walkable state indices.\"\"\"\n",
    "    state_indices = []\n",
    "    for y in range(env.height):\n",
    "        for x in range(env.width):\n",
    "            if env.grid.get(x, y) is None:  # Empty/walkable\n",
    "                state_indices.append(position_to_state_index((x, y)))\n",
    "    return state_indices\n",
    "\n",
    "allowed_state_idx = find_state_indexes()\n",
    "\n",
    "num_actions = 5\n",
    "num_states = (env.width) * (env.height)\n",
    "\n",
    "\n",
    "def next_state_index(current_state_idx, action):\n",
    "    \"\"\"Given a state index and action, return the resulting state index.\"\"\"\n",
    "    x, y = state_index_to_position(current_state_idx)\n",
    "\n",
    "    # Movement logic\n",
    "    if action == 0:  # stay\n",
    "        pass\n",
    "    elif action == 1:  # up\n",
    "        y -= 1\n",
    "    elif action == 2:  # down\n",
    "        y += 1\n",
    "    elif action == 3:  # left\n",
    "        x -= 1\n",
    "    elif action == 4:  # right\n",
    "        x += 1\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid action: {action}\")\n",
    "\n",
    "    \n",
    "    # Convert the new (x, y, direction) back to a state index.\n",
    "    next_state = position_to_state_index((x, y))\n",
    "    if next_state not in allowed_state_idx:\n",
    "        # If the next state is not allowed, return the current state\n",
    "        return current_state_idx\n",
    "    return next_state\n",
    "P_s_given_s_a = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "def find_all_next_states():\n",
    "    \"\"\"Find all possible next states for each state and action.\"\"\"\n",
    "    for state in allowed_state_idx:\n",
    "        for action in range(num_actions):\n",
    "            next_state = next_state_index(state, action)\n",
    "            P_s_given_s_a[state, action, next_state] = 1 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Initialized grid 69x37, agent at (1, 30)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2000\u001b[39m):\n\u001b[1;32m      4\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# random action\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# 5-tuple\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Current state index from current (x,y,dir)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     idx \u001b[38;5;241m=\u001b[39m position_to_state_index()\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "for i in range(2000):\n",
    "    action = env.action_space.sample()  # random action\n",
    "        \n",
    "    obs, reward, terminated, truncated, info = env.step(action)  # 5-tuple\n",
    "\n",
    "    # Current state index from current (x,y,dir)\n",
    "    idx = position_to_state_index()\n",
    "    x, y, d = state_index_to_position(idx)\n",
    "    idx = position_to_state_index()\n",
    "    print(position_to_state_index())\n",
    "    print(state_index_to_position(idx))\n",
    "    \n",
    "    \n",
    "    print(f\"Step {i}: action={action}, pos={env.agent_pos}, reward={reward}\")\n",
    "    env.render()  # <- show movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model State Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "\n",
    "class ModelState:\n",
    "    #tables and arrays\n",
    "    Q_table: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    Pi_a_s: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    # P_s_given_s_a: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    P_s_by_s: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    \n",
    "    allowed_state_idx: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    \n",
    "    #scalars\n",
    "    beta: Optional[float] = None\n",
    "    num_actions: Optional[int] = None\n",
    "    num_states: Optional[int] = None\n",
    "\n",
    "    #lists\n",
    "    info_to_go_term_training: List[float] = field(default_factory=list)\n",
    "    pi_analysis_term_training: List[float] = field(default_factory=list)\n",
    "\n",
    "    #list for P_s\n",
    "    positions_directions_list_ps: List[float] = field(default_factory=list)\n",
    "    positions_directions_list_neglogps: List[float] = field(default_factory=list)\n",
    "\n",
    "    #dictionaries\n",
    "    transition_counts: Dict = field(default_factory=dict)\n",
    "    connect_dict: Dict = field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate state instance, can also import from another file if needed\n",
    "state = ModelState()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMixin:\n",
    "    def position_to_state_index(self, pos=None):\n",
    "        \"\"\"Map (x, y) to state index assuming row-major order.\"\"\"\n",
    "        if pos is None:\n",
    "            x, y = self.env.agent_pos\n",
    "        else:\n",
    "            x, y = pos\n",
    "        return y * self.env.width + x\n",
    "\n",
    "    def state_index_to_position(self, idx):\n",
    "        \"\"\"Map scalar state index to (x, y) position.\"\"\"\n",
    "        x = idx % self.env.width\n",
    "        y = idx // self.env.width\n",
    "        return x, y\n",
    "\n",
    "    def find_state_indexes(self):\n",
    "        \"\"\"Return all non-wall, walkable state indices.\"\"\"\n",
    "        state_indices = []\n",
    "        for y in range(self.env.height):\n",
    "            for x in range(self.env.width):\n",
    "                if self.env.grid.get(x, y) is None:  # Empty/walkable\n",
    "                    state_indices.append(self.position_to_state_index((x, y)))\n",
    "        return state_indices\n",
    "\n",
    "    def next_state_index(self, current_state_idx, action):\n",
    "        \"\"\"Given a state index and action, return the resulting state index.\"\"\"\n",
    "        x, y = self.state_index_to_position(current_state_idx)\n",
    "\n",
    "        # Movement logic\n",
    "        if action == 0:  # stay\n",
    "            pass\n",
    "        elif action == 1:  # up\n",
    "            y -= 1\n",
    "        elif action == 2:  # down\n",
    "            y += 1\n",
    "        elif action == 3:  # left\n",
    "            x -= 1\n",
    "        elif action == 4:  # right\n",
    "            x += 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "\n",
    "        \n",
    "        # Convert the new (x, y, direction) back to a state index.\n",
    "        next_state = self.position_to_state_index((x, y))\n",
    "        if next_state not in self.state.allowed_state_idx:\n",
    "            # If the next state is not allowed, return the current state\n",
    "            return current_state_idx\n",
    "        return next_state\n",
    "    \n",
    "    \n",
    "    def find_all_next_states(self):\n",
    "        \"\"\"Find all possible next states for each state and action.\"\"\"\n",
    "        self.state.connect_dict = {}\n",
    "        for state in self.state.allowed_state_idx:\n",
    "            tmp = []\n",
    "            for action in range(self.state.num_actions):\n",
    "                next_state = self.next_state_index(state, action)\n",
    "                tmp.append((next_state, action))\n",
    "            self.state.connect_dict[state] = tmp\n",
    "\n",
    "            \n",
    "\n",
    "    def find_connected_states(self):\n",
    "        \"\"\"\n",
    "        Build and return a connectivity matrix P_s_by_s where the element at [state, next_state]\n",
    "        is given by the probability from self.Pi_a_s for the action that leads from state to next_state.\n",
    "        \"\"\"\n",
    "        # Reset the connectivity matrix at the beginning.\n",
    "        # self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states))\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states))\n",
    "        # Loop over all states.\n",
    "        for state in self.state.allowed_state_idx:\n",
    "            # Loop over all actions for the state.\n",
    "            for pair in self.state.connect_dict[state]:\n",
    "                s_n, action = pair\n",
    "                # Set the connectivity matrix: you might choose to sum if multiple actions lead to the same state.\n",
    "                self.state.P_s_by_s[state, s_n] += self.state.Pi_a_s[state, action]\n",
    "    \n",
    "    def update_connected_states(self, s):\n",
    "        \"\"\"Update the connectivity matrix P_s_by_s for a single state s.\"\"\"\n",
    "        for pair in self.state.connect_dict[s]:\n",
    "                s_n, action = pair\n",
    "                # Set the connectivity matrix: you might choose to sum if multiple actions lead to the same state.\n",
    "                self.state.P_s_by_s[s, s_n] = self.state.Pi_a_s[s, action]\n",
    "\n",
    "    \n",
    "    def propagate_distribution(self, p0, A, steps):\n",
    "        \"\"\"\n",
    "        Propagate initial distribution p0 through transition matrix A\n",
    "        for given number of steps.\n",
    "        A is CSR sparse matrix, row-stochastic.\n",
    "        \"\"\"\n",
    "        if steps == 0:\n",
    "            return p0.copy()\n",
    "        else:\n",
    "            p = p0.copy()\n",
    "            for _ in range(steps):\n",
    "                p = p @ A  # vector * sparse matrix\n",
    "            return p\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################working code! ######################\n",
    "class FreeEnergy(GridMixin):\n",
    "    def __init__(self, env, state: ModelState, epochs = 200, beta = 0.16, goal_state = None):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        state: ModelState\n",
    "        self.goal_state = goal_state   # Default goal state if not provided\n",
    "        self.reward_mask = env.reward_mask\n",
    " \n",
    "        # self.max_steps = 100\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        self.state = state\n",
    "        self.state.allowed_state_idx = self.find_state_indexes()\n",
    "        self.state.num_states = env.width * env.height\n",
    "        print(f\"Number of states: {self.state.num_states}\")\n",
    "        self.state.num_actions = 5\n",
    "        #shapes defined for first time\n",
    "        # self.state.P_s_given_s_a = np.zeros((self.state.num_states, self.state.num_actions, self.state.num_states)).astype(int) # P(s'|s,a) matrix\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states)) # P(s'|s) matrix\n",
    "        self.state.Pi_a_s = np.full((self.state.num_states, self.state.num_actions), 1/self.state.num_actions) # pi(s,a) matrix\n",
    "      \n",
    "        self.Free_energy_table = np.full((self.state.num_states, self.state.num_actions), 0.0) # Free energy table\n",
    "\n",
    "\n",
    "        \n",
    "        self.state.beta = beta\n",
    "\n",
    "        \n",
    "        self.Pi_a = np.full((self.state.num_actions), 1/self.state.num_actions)\n",
    "        self.P_s = np.zeros(self.state.num_states)\n",
    "        \n",
    "       \n",
    "    \"\"\" Perform training using BA updates on the Probabilistic Q Learning System \"\"\"\n",
    "\n",
    "\n",
    "    def train_BA(self):\n",
    "        max_ba_iters = 10 #max iterations for the Blahut-Arimoto update\n",
    "        tol = 1e-2  #tolerance for convergence\n",
    "     #tolerance for convergence\n",
    "        self.find_all_next_states() #generates self.P(s'|s,a) matrix\n",
    "        \n",
    "        self.find_connected_states() #P_s fixed by old Pi_a_s\n",
    "\n",
    "        \n",
    "        goal_states = [self.position_to_state_index((self.goal_state))]\n",
    "        \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.steps = 0\n",
    "            #self.env.reset()[0] #can reset the environment to start from the beginning\n",
    "            current_state = np.random.choice(self.state.allowed_state_idx) #self.position_to_state_index()\n",
    "            x,y = self.state_index_to_position(current_state)\n",
    "            self.env.agent_pos = (x,y)\n",
    "            # self.env.agent_dir = dir\n",
    "\n",
    "            initial_s = np.zeros(self.state.num_states)\n",
    "            initial_s[current_state] = 1\n",
    "\n",
    "            self.visited_reward_0 = set()\n",
    "\n",
    "            while current_state not in goal_states:  # and self.steps < self.max_steps:\n",
    "                #get the action using pi\n",
    "                action = np.random.choice(np.arange(self.state.num_actions), p = self.state.Pi_a_s[current_state])\n",
    "                #transition to the next state\n",
    "                # switched from 5 to 4 \n",
    "                next_obs, _, done, _ = self.env.step(action)\n",
    "\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index()\n",
    "\n",
    "                if self.goal_state is None:\n",
    "                    reward = 0 if next_state in goal_states else -1\n",
    "                else:\n",
    "                    x, y = self.state_index_to_position(next_state)  # or self.env.state_index_to_position\n",
    "                    if self.reward_mask[y, x] == 0:\n",
    "                        if (x, y) in self.visited_reward_0:\n",
    "                            reward = -1\n",
    "                        else:\n",
    "                            reward = 0\n",
    "                            self.visited_reward_0.add((x, y))\n",
    "                    else:\n",
    "                        reward = -1\n",
    "             \n",
    "\n",
    "                sparse_s_by_s = csr_matrix(self.state.P_s_by_s)\n",
    "                P_s = self.propagate_distribution(p0 = initial_s, A = sparse_s_by_s, steps = self.steps)\n",
    "\n",
    "                # Ps_s_matrix = matrix_power(self.state.P_s_by_s, self.steps)  # Convert dia → csr\n",
    "                # P_s = initial_s @ Ps_s_matrix\n",
    "                \n",
    "                # P_s /= P_s.sum() #normalise\n",
    "           \n",
    "                \n",
    "                assert np.isclose(np.sum(P_s), 1, atol= 1e-8), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "                \n",
    "                #calculate the Free energy\n",
    "                F_0 = (-np.log(P_s[next_state]+ 1e-15)) - (self.beta * reward)\n",
    "                # print(f\" P_s[current_state]: {P_s[current_state]}\")\n",
    "                # print (f\"P_s[next_state]: {P_s[next_state]}\")\n",
    "            \n",
    "                G_0 = np.sum((np.log(self.state.Pi_a_s[next_state] + 1e-15) - np.log(self.Pi_a+1e-15) + self.Free_energy_table[next_state]) * self.state.Pi_a_s[next_state])\n",
    "\n",
    "                F_0 += G_0 \n",
    "\n",
    "                self.Free_energy_table[current_state, action] = F_0 #np.min([F_0, 200]) #    #np.min([F_0, 50])\n",
    "                #self.Free_energy_table[self.state.allowed_state_idx] = self.Free_energy_table[self.state.allowed_state_idx] - np.mean(self.Free_energy_table[self.state.allowed_state_idx])\n",
    "                # print(f\"F_0:{F_0}\")\n",
    "                    # self.Free_energy_table[self.state.allowed_state_idx] = self.Free_energy_table[self.state.allowed_state_idx] - np.mean(self.Free_energy_table[self.state.allowed_state_idx])\n",
    "                \n",
    "                \n",
    "\n",
    "                #######loop of calculations#####\n",
    "                for iteration in range(max_ba_iters):\n",
    "                  \n",
    "\n",
    "                    self.Pi_a = P_s@self.state.Pi_a_s \n",
    "                \n",
    "                  \n",
    "                    \n",
    "                    assert np.isclose(np.sum(self.Pi_a), 1), f\"Sum of Pi_a is not 1: {np.sum(self.Pi_a)}\"\n",
    "\n",
    "                    \n",
    "                    # 1) log π(a)  — shape (A,)\n",
    "                    log_pi_a = np.log(self.Pi_a + 1e-15)\n",
    "\n",
    "                    # 2) log-joint   log π(a) − β F(s,a)  — broadcast to shape (S,A)\n",
    "                    log_joint = log_pi_a  -self.Free_energy_table\n",
    "\n",
    "                    # 3) row-shift: subtract max in each state to keep exp() ≤ 1\n",
    "                    log_joint -= log_joint.max(axis=1, keepdims=True)\n",
    "\n",
    "                    # 4) exponentiate and normalise each row\n",
    "                    new_Pi_a_s = np.exp(log_joint)\n",
    "                    new_Pi_a_s /= new_Pi_a_s.sum(axis=1, keepdims=True)   # Σ_a π(a|s)=1\n",
    "                                       \n",
    "                    #calculate Z, works only for small beta\n",
    "                    # element_wise_a_by_q_table = self.Pi_a * np.exp(-  self.Free_energy_table)\n",
    "                    \n",
    "                    # zeta = np.sum((element_wise_a_by_q_table), axis = 1)\n",
    "                   \n",
    "                    \n",
    "                    # #calculate for policy using partition fuction\n",
    "\n",
    "                    # new_Pi_a_s = (self.Pi_a * np.exp(-  self.Free_energy_table))/ zeta.reshape(-1,1) \n",
    "                    diff = np.linalg.norm(new_Pi_a_s - self.state.Pi_a_s, ord='fro')\n",
    "                    # print(f\"Epoch {epoch}: Frobenius norm difference: {diff:.4f}\") # with percent\n",
    "                    \n",
    "                    self.state.Pi_a_s = new_Pi_a_s\n",
    "                    iteration += 1\n",
    "                    if diff < tol:\n",
    "                        # print(f\"Convergence reached at epoch {epoch} \"f\"after {iteration} BA iteration(s); \"f\"Δ = {diff:.4e}\")\n",
    "                        break\n",
    "\n",
    "                      # Frobenius norm for matrix difference\n",
    "                # self.state.P_s_by_s = P_s_csr.tolil()\n",
    "                self.update_connected_states(current_state)\n",
    "\n",
    "                # #update state if only single goal state \n",
    "                if self.goal_state is not None and next_state in goal_states:\n",
    "                    # print(f\"Goal reached at state {next_state} in epoch {epoch} after {self.steps} steps.\")\n",
    "                    break\n",
    "                # assert(False)\n",
    "                current_state = next_state\n",
    "                # print(f\"Epoch {epoch}: Step {self.steps}, Current state: {current_state}\")\n",
    "                self.steps += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "##################working code! ######################\n",
    "class FreeEnergy(GridMixin):\n",
    "    def __init__(self, env, state: ModelState, epochs = 20, beta = 5):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        state: ModelState\n",
    "         # self.max_steps = 100\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        self.state = state\n",
    "        self.state.allowed_state_idx = self.find_state_indexes()\n",
    "        self.state.num_states = env.width * env.height\n",
    "        self.state.num_actions = 5\n",
    "        #shapes defined for first time\n",
    "        # self.state.P_s_given_s_a = np.zeros((self.state.num_states, self.state.num_actions, self.state.num_states)) # P(s'|s,a) matrix\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states)) # P(s'|s) matrix\n",
    "        self.state.Pi_a_s = np.full((self.state.num_states, self.state.num_actions), 1/self.state.num_actions) # pi(s,a) matrix\n",
    "      \n",
    "        self.Free_energy_table = np.full((self.state.num_states, self.state.num_actions), 0.0) # Free energy table\n",
    "\n",
    "\n",
    "        self.reward_mask = reward_struct\n",
    "        self.state.beta = beta\n",
    "\n",
    "        \n",
    "        self.Pi_a = np.full((self.state.num_actions), 1/self.state.num_actions)\n",
    "        self.P_s = np.zeros(self.state.num_states)\n",
    "        self.visited_reward_0 = set()  # holds (x, y) tuples\n",
    "\n",
    "    def train_BA(self):\n",
    "        max_ba_iters = 10\n",
    "        tol = 1e-2\n",
    "\n",
    "        goal_states = [self.position_to_state_index((30, 15))]  # Goal at (30, 15)\n",
    "       \n",
    "        \n",
    "\n",
    "        self.find_all_next_states() #generates self.P(s'|s,a) matrix\n",
    "        \n",
    "        self.find_connected_states() #P_s fixed by old Pi_a_s\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch }/{self.epochs} starting...\")\n",
    "            self.steps = 0\n",
    "            current_state = np.random.choice(self.state.allowed_state_idx)  #2071\n",
    "            #print\n",
    "            # print(f\"current state: {current_state}\")\n",
    "            x, y = self.state_index_to_position(current_state)\n",
    "            # print(f\"current position: {x}, {y}\")\n",
    "            self.env.agent_pos = (x, y)\n",
    "            # print(f\"current agent position: {self.env.agent_pos}\")\n",
    "          \n",
    "\n",
    "            initial_s = np.zeros(self.state.num_states)\n",
    "            initial_s[current_state] = 1\n",
    "\n",
    "            while self.steps < 150:  # ← Now limits to 100 steps per epoch\n",
    "                action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[current_state])\n",
    "                # print(f\"Step {self.steps}: action={action}, current_state={current_state}\")\n",
    "                _, _, _, _ = self.env.step(action)\n",
    "                # self.env.render()\n",
    "                next_state = self.position_to_state_index()\n",
    "                # print(f\"next state: {next_state}\")\n",
    "\n",
    "                # reward = self.env.reward_mask[self.env.agent_pos[1], self.env.agent_pos[0]] \\\n",
    "                #         if self.env.reward_mask is not None else 0\n",
    "\n",
    "                # if self.reward_mask[self.env.agent_pos[1], self.env.agent_pos[0]] == 0:\n",
    "                #     if (self.env.agent_pos[0], self.env.agent_pos[1]) in self.visited_reward_0:\n",
    "                #         reward = -1  # revisited exhibit\n",
    "                #     else:\n",
    "                #         reward = 0   # first time visit\n",
    "                #         self.visited_reward_0.add((self.env.agent_pos[0], self.env.agent_pos[1]))\n",
    "                # else:\n",
    "                #     reward = self.reward_mask[self.env.agent_pos[1], self.env.agent_pos[0]]\n",
    "\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "\n",
    "                Ps_s_matrix = matrix_power(self.state.P_s_by_s, self.steps)  # Convert dia → csr\n",
    "        \n",
    "         \n",
    "                P_s = initial_s @ Ps_s_matrix\n",
    "                # P_s /= P_s.sum() #normalise\n",
    "           \n",
    "                \n",
    "                assert np.isclose(np.sum(P_s), 1, atol= 1e-8), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "\n",
    "             \n",
    "                F_0 = (-np.log(P_s[next_state]+ 1e-15)) - (self.beta * reward)\n",
    "                # print(f\" P_s[current_state]: {P_s[current_state]}\")\n",
    "                # print (f\"P_s[next_state]: {P_s[next_state]}\")\n",
    "            \n",
    "                G_0 = np.sum((np.log(self.state.Pi_a_s[next_state] + 1e-15) - np.log(self.Pi_a+1e-15) + self.Free_energy_table[next_state]) * self.state.Pi_a_s[next_state])\n",
    "\n",
    "                F_0 += G_0 \n",
    "\n",
    "                self.Free_energy_table[current_state, action] = F_0 #np.min([F_0, 200]) #    #np.min([F_0, 50])\n",
    "\n",
    "                #######loop of calculations#####\n",
    "                for iteration in range(max_ba_iters):\n",
    "                  \n",
    "\n",
    "                    self.Pi_a = P_s@self.state.Pi_a_s \n",
    "                \n",
    "                  \n",
    "                    \n",
    "                    assert np.isclose(np.sum(self.Pi_a), 1), f\"Sum of Pi_a is not 1: {np.sum(self.Pi_a)}\"\n",
    "\n",
    "                    \n",
    "                    # 1) log π(a)  — shape (A,)\n",
    "                    log_pi_a = np.log(self.Pi_a + 1e-15)\n",
    "\n",
    "                    # 2) log-joint   log π(a) − β F(s,a)  — broadcast to shape (S,A)\n",
    "                    log_joint = log_pi_a  -self.Free_energy_table\n",
    "\n",
    "                    # 3) row-shift: subtract max in each state to keep exp() ≤ 1\n",
    "                    log_joint -= log_joint.max(axis=1, keepdims=True)\n",
    "\n",
    "                    # 4) exponentiate and normalise each row\n",
    "                    new_Pi_a_s = np.exp(log_joint)\n",
    "                    new_Pi_a_s /= new_Pi_a_s.sum(axis=1, keepdims=True)   # Σ_a π(a|s)=1\n",
    "                                       \n",
    "                    #calculate Z, works only for small beta\n",
    "                    # element_wise_a_by_q_table = self.Pi_a * np.exp(-  self.Free_energy_table)\n",
    "                    \n",
    "                    # zeta = np.sum((element_wise_a_by_q_table), axis = 1)\n",
    "                   \n",
    "                    \n",
    "                    # #calculate for policy using partition fuction\n",
    "\n",
    "                    # new_Pi_a_s = (self.Pi_a * np.exp(-  self.Free_energy_table))/ zeta.reshape(-1,1) \n",
    "                    diff = np.linalg.norm(new_Pi_a_s - self.state.Pi_a_s, ord='fro')\n",
    "                    # print(f\"Epoch {epoch}: Frobenius norm difference: {diff:.4f}\") # with percent\n",
    "                    \n",
    "                    self.state.Pi_a_s = new_Pi_a_s\n",
    "                    iteration += 1\n",
    "                    if diff < tol:\n",
    "                        # print(f\"Convergence reached at epoch {epoch} \"f\"after {iteration} BA iteration(s); \"f\"Δ = {diff:.4e}\")\n",
    "                        break\n",
    "\n",
    "                      # Frobenius norm for matrix difference\n",
    "\n",
    "                self.update_connected_states(current_state)\n",
    "\n",
    "                 #update state\n",
    "                if next_state in goal_states:\n",
    "                    # print(f\"Goal reached at state {next_state} in epoch {epoch} after {self.steps} steps.\")\n",
    "                    break\n",
    "\n",
    "                # assert(False)\n",
    "                current_state = next_state\n",
    "                # print(f\"Epoch {epoch}: Step {self.steps}, Current state: {current_state}\")\n",
    "                self.steps += 1\n",
    "                # print(f\"Epoch {epoch}: Step {self.steps}, Current state: {current_state}, Action: {action}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Initialized grid 69x37, agent at (1, 30)\n"
     ]
    }
   ],
   "source": [
    "env = SimpleEnv(render_mode= None, mask = mask, reward_mask= reward_struct)\n",
    "#env.reset needed to bypass the step counter function which is otherwise needed but doesn't exist in SimpleEnv\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "free = FreeEnergy(env, state, beta = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Sum of P(s) is not 1: 0.9959207624190647",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_BA\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 94\u001b[0m, in \u001b[0;36mFreeEnergy.train_BA\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m P_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate_distribution(p0 \u001b[38;5;241m=\u001b[39m initial_s, A \u001b[38;5;241m=\u001b[39m sparse_s_by_s, steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Ps_s_matrix = matrix_power(self.state.P_s_by_s, self.steps)  # Convert dia → csr\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# P_s = initial_s @ Ps_s_matrix\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# P_s /= P_s.sum() #normalise\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(np\u001b[38;5;241m.\u001b[39msum(P_s), \u001b[38;5;241m1\u001b[39m, atol\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-8\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of P(s) is not 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(P_s)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#calculate the Free energy\u001b[39;00m\n\u001b[1;32m     97\u001b[0m F_0 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(P_s[next_state]\u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-15\u001b[39m)) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m*\u001b[39m reward)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Sum of P(s) is not 1: 0.9959207624190647"
     ]
    }
   ],
   "source": [
    "free.train_BA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Initialized grid 69x37, agent at (np.int64(7), np.int64(11))\n"
     ]
    }
   ],
   "source": [
    "test_env = SimpleEnv(\n",
    "    agent_start_pos= None,     # (x, y) format → left edge, near bottom           # Facing up (0 = up, 1 = right, 2 = down, 3 = left)\n",
    "    render_mode=\"human\",\n",
    "    mask=mask,\n",
    "    reward_mask=reward_struct\n",
    ")\n",
    "test_env.reset(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.63895302e-06, 1.01031555e-15, 6.43036042e-09, 9.99988369e-01,\n",
       "       9.98522528e-06])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.Pi_a_s[1303]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRunner(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        \n",
    "    \n",
    "    def run_policy_2(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q-table.\"\"\"\n",
    "\n",
    "        # self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        self.step_count = 0  # Track steps to prevent infinite loops\n",
    "\n",
    "        while not done and self.step_count < 1000:  # Prevent infinite loops\n",
    "  \n",
    "            action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[current_state])  # Choose best action\n",
    "            next_obs, _, done, _ = self.env.step(action)  # Take action\n",
    "            next_state = self.position_to_state_index()  # Convert new state\n",
    "            self.env.render()  # Visualize movement\n",
    "            # Calculate info to go term for the current step\n",
    "            print(f\"current state: {current_state}, action: {action}, next state: {next_state}\")\n",
    "            current_state = next_state  # Update current state\n",
    "            self.step_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = QRunner(test_env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 13)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_index_to_position(951)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n",
      "current state: 766, action: 0, next state: 766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[381], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_policy_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[331], line 20\u001b[0m, in \u001b[0;36mQRunner.run_policy_2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m next_obs, _, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Take action\u001b[39;00m\n\u001b[1;32m     19\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_to_state_index()  \u001b[38;5;66;03m# Convert new state\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Visualize movement\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate info to go term for the current step\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_state\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, next state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_state\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/IBP/lib/python3.11/site-packages/minigrid/minigrid_env.py:781\u001b[0m, in \u001b[0;36mMiniGridEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mblit(bg, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    780\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 781\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner.run_policy_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r state_lookup_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_lookup = state_lookup_03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
